{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Function finding embedding with highest cosine similarity"],"metadata":{"id":"ah_Pl8swjU6E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF38A5pjjPGr"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def find_most_similar_large(arr, predicted_embedding):\n","    \"\"\"\n","    Find the text and embedding that is most similar to the predicted embedding in a large dataset.\n","\n","    Parameters:\n","    - arr (array) : array containing the  text and embeddings\n","    - predicted_embedding (np.ndarray): The embedding to compare against.\n","\n","    Returns:\n","    - dict: A dictionary with 'text', 'embedding', and 'similarity' of the most similar entry.\n","    \"\"\"\n","    df = pd.DataFrame(arr, columns=['text', 'embedding'])\n","    predicted_embedding = np.array(predicted_embedding).reshape(1, -1)\n","    max_similarity = -1\n","    most_similar_entry = None\n","\n","    for row in df:\n","        text, embedding = row['text'], np.array(row['embedding'])\n","        similarity = cosine_similarity(embedding.reshape(1, -1), predicted_embedding).item()\n","\n","        if similarity > max_similarity:\n","            max_similarity = similarity\n","            most_similar_entry = {'text': text, 'embedding': embedding, 'similarity': similarity}\n","\n","    return most_similar_entry"]},{"cell_type":"markdown","source":["### Function for extracting top 10 embeddings and then top 10 products"],"metadata":{"id":"l_Gt7ilQjdVQ"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import heapq\n","\n","def find_top_k_similar_large(arr, predicted_embedding, k=10):\n","    \"\"\"\n","    Find the top K most similar text and embeddings to the predicted embedding in a large dataset.\n","\n","    Parameters:\n","    - arr (array) : array containing the text and embeddings\n","    - predicted_embedding (np.ndarray): The embedding to compare against.\n","    - k (int): Number of top similar entries to return.\n","\n","    Returns:\n","    - list of dict: A list of dictionaries with 'text', 'embedding', and 'similarity' for the top K entries.\n","    \"\"\"\n","    df = pd.DataFrame(arr_prod, columns=['text', 'embedding'])\n","    predicted_embedding = np.array(predicted_embedding).reshape(1, -1)  # Reshape for compatibility\n","    top_k = []  # Min-heap to store top K entries\n","\n","    for _, row in df.iterrows():\n","        text, embedding = row['text'], np.array(row['embedding'])\n","        similarity = cosine_similarity(embedding.reshape(1, -1), predicted_embedding).item()\n","\n","        # Maintain a heap of size k for top K similarities\n","        if len(top_k) < k:\n","            heapq.heappush(top_k, (similarity, text, embedding))\n","        else:\n","            heapq.heappushpop(top_k, (similarity, text, embedding))\n","\n","    # Sort by similarity in descending order\n","    top_k = sorted(top_k, key=lambda x: x[0], reverse=True)\n","    return [(t, e) for t, e in top_k]\n","\n"],"metadata":{"id":"PEee3QAvjUEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_product_ids_for_embeddings(arr, arr_prod, predicted_embedding, max_ids=10):\n","    \"\"\"\n","    For each text-embedding pair in the dictionary, search the dataframe for the most similar text\n","    and retrieve corresponding product_ids, ensuring no repetitions. The process continues until\n","    the list contains 'max_ids' unique product_ids.\n","\n","    Parameters:\n","    - arr (array) : array containing the  text and embeddings\n","    - arr_prod (array) : array containing the embeddings and product_ids\n","    - max_ids (int): The maximum number of unique product_ids to retrieve.\n","\n","    Returns:\n","    - list: A list of unique product_ids based on cosine similarity.\n","    \"\"\"\n","    df = pd.DataFrame(arr_prod, columns=['embedding', 'product_ids'])\n","    text_embedding_list = find_top_k_similar_large(arr, predicted_embedding, k=10)\n","    unique_product_ids = set()  # To store unique product_ids\n","    embeddings_df = np.vstack(df['embedding'].values)  # Assuming 'embedding' is a list of embeddings in df\n","\n","    # Iterate over each text-embedding tuple in the list\n","    for i in range(len(text_embedding_list:))\n","      for text, embedding in text_embedding_list[i]:\n","\n","          if len(unique_product_ids) >= max_ids:\n","              break  # Stop if we've already collected 'max_ids' product_ids\n","\n","          # Compute cosine similarities between the current text embedding and all embeddings in the DataFrame\n","          embedding = np.array(embedding).reshape(1, -1)  # Reshape for compatibility\n","          similarities = cosine_similarity(embeddings_df, embedding).flatten()\n","\n","          # Get the indices of the most similar entries\n","          similar_indices = np.argsort(similarities)[::-1]\n","\n","          # Iterate through the sorted indices and collect unique product_ids\n","          for idx in similar_indices:\n","            if len(unique_product_ids) >= max_ids:\n","                break  # Stop once we reach the limit\n","\n","            # Extract and split product IDs (assumes product_ids is a comma-separated string)\n","            product_ids = df.iloc[idx]['product_ids'].split(',')\n","            unique_product_ids.update(product_ids)  # Add multiple IDs to the set at once\n","\n","    # Create a ranked list of product IDs\n","    ranked_product_ids = [(product_id, rank + 1) for rank, product_id in enumerate(unique_product_ids)]\n","    return ranked_product_ids"],"metadata":{"id":"EJqKft7yjUBW"},"execution_count":null,"outputs":[]}]}